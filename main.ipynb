{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "83f3287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import string\n",
    "import emoji\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5186b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fff63c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Date Created</th>\n",
       "      <th>Number of Likes</th>\n",
       "      <th>Source of Tweet</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1890053744647401717</td>\n",
       "      <td>2025-02-13 15:01:42+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tinjau Program Makan Bergizi Gratis, Danseskoa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1890053689223876827</td>\n",
       "      <td>2025-02-13 15:01:29+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>üçΩÔ∏è‚ú® Program Makan Bergizi Gratis dapat meningk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1890053643682070881</td>\n",
       "      <td>2025-02-13 15:01:18+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Makin ngga jelas aja ini negara demi makan sia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1890053393311478011</td>\n",
       "      <td>2025-02-13 15:00:19+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"pa, siang tadi adek makan gratis.\"\\n\\n\"dek, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1890053355197837730</td>\n",
       "      <td>2025-02-13 15:00:10+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Makan siang gratis https://t.co/FF7x4tFkyt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>1893239777601667083</td>\n",
       "      <td>2025-02-22 10:01:52+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hasil dari School Meal Program, atau \"Pradan M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1893238494870257791</td>\n",
       "      <td>2025-02-22 09:56:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Baznas Siap Dukung Program Makan Bergizi Grati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>1893238009912295536</td>\n",
       "      <td>2025-02-22 09:54:50+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Makan bergizi gratis yang sebenernya gak grati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1893237952790069460</td>\n",
       "      <td>2025-02-22 09:54:37+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Program makan bergizi gratis memberikan manfaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>1893237927406112815</td>\n",
       "      <td>2025-02-22 09:54:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Program makan bergizi gratis memberikan manfaa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tweet ID               Date Created  Number of Likes  \\\n",
       "0    1890053744647401717  2025-02-13 15:01:42+00:00                0   \n",
       "1    1890053689223876827  2025-02-13 15:01:29+00:00                0   \n",
       "2    1890053643682070881  2025-02-13 15:01:18+00:00                1   \n",
       "3    1890053393311478011  2025-02-13 15:00:19+00:00                0   \n",
       "4    1890053355197837730  2025-02-13 15:00:10+00:00                0   \n",
       "..                   ...                        ...              ...   \n",
       "886  1893239777601667083  2025-02-22 10:01:52+00:00                2   \n",
       "887  1893238494870257791  2025-02-22 09:56:46+00:00                0   \n",
       "888  1893238009912295536  2025-02-22 09:54:50+00:00                0   \n",
       "889  1893237952790069460  2025-02-22 09:54:37+00:00                0   \n",
       "890  1893237927406112815  2025-02-22 09:54:31+00:00                0   \n",
       "\n",
       "     Source of Tweet                                              Tweet  \n",
       "0                NaN  Tinjau Program Makan Bergizi Gratis, Danseskoa...  \n",
       "1                NaN  üçΩÔ∏è‚ú® Program Makan Bergizi Gratis dapat meningk...  \n",
       "2                NaN  Makin ngga jelas aja ini negara demi makan sia...  \n",
       "3                NaN  \"pa, siang tadi adek makan gratis.\"\\n\\n\"dek, m...  \n",
       "4                NaN         Makan siang gratis https://t.co/FF7x4tFkyt  \n",
       "..               ...                                                ...  \n",
       "886              NaN  hasil dari School Meal Program, atau \"Pradan M...  \n",
       "887              NaN  Baznas Siap Dukung Program Makan Bergizi Grati...  \n",
       "888              NaN  Makan bergizi gratis yang sebenernya gak grati...  \n",
       "889              NaN  Program makan bergizi gratis memberikan manfaa...  \n",
       "890              NaN  Program makan bergizi gratis memberikan manfaa...  \n",
       "\n",
       "[891 rows x 5 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f17762d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop(columns=['Tweet ID', 'Date Created', 'Number of Likes', 'Source of Tweet'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cb1aee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dict = {\n",
    "    \"kaga\": \"tidak\",\n",
    "    \"ngga\": \"tidak\",\n",
    "    \"gada\": \"tidak\",\n",
    "    \"gak\": \"tidak\",\n",
    "    \"ga\": \"tidak\",\n",
    "    \"yg\": \"yang\",\n",
    "    \"udah\": \"sudah\",\n",
    "    \"gue\": \"saya\",\n",
    "    \"lu\": \"kamu\",\n",
    "    \"lo\": \"kamu\",\n",
    "    \"gw\": \"saya\",\n",
    "    \"tuh\": \"itu\",\n",
    "    \"nih\": \"ini\",\n",
    "    \"tp\": \"tapi\",\n",
    "    \"dr\": \"dari\",\n",
    "    \"pd\": \"pada\",\n",
    "    \"gw\": \"saya\",\n",
    "    \"kalo\": \"kalau\",\n",
    "    \"si\": \"\",\n",
    "    \"btw\": \"ngomong-ngomong\",\n",
    "    \"dapet\": \"dapat\",\n",
    "    \"sampe\": \"sampai\",\n",
    "    \"mending\": \"lebih baik\",\n",
    "    \"amp\": \"dan\",\n",
    "    \"aja\": \"saja\",\n",
    "    \"pas\": \"saat\",\n",
    "    \"kayak\": \"seperti\",\n",
    "    \"bilang\": \"mengatakan\",\n",
    "    \"duit\": \"uang\",\n",
    "    \"pinter\": \"pintar\",\n",
    "    \"sih\": \"\",\n",
    "    \"oke\": \"baik\",\n",
    "    \"bikin\": \"membuat\",\n",
    "    \"boong\": \"bohong\",\n",
    "    \"pinter\": \"pintar\",\n",
    "    \"no\": \"tidak\",\n",
    "    \"males\": \"malas\", \n",
    "    \"episiensi\": \"efisiensi\",\n",
    "    \"anjir\": \"anjing\",\n",
    "    \"bgst\": \"bangsat\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "76caefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r\":\\w+:\", \"\", text)\n",
    "    text = re.sub(r'¬≤', '', text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text, slang_dict):\n",
    "    # Cleaning\n",
    "    text = text_cleaning(text)\n",
    "    \n",
    "    # Tokenization + Slang Conversion + Stopwords Removal + Stemming\n",
    "    stop_words = set(stopwords.words(\"indonesian\"))\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [slang_dict.get(word, word) for word in tokens]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "72fd99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['Clean Text'] = tweets_df['Tweet'].apply(text_cleaning)\n",
    "tweets_df = tweets_df.drop_duplicates(subset='Clean Text').reset_index(drop=True)\n",
    "tweets_df['Preprocess'] = tweets_df['Clean Text'].apply(lambda x: preprocess(x, slang_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "85af7fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Clean Text</th>\n",
       "      <th>Preprocess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tinjau Program Makan Bergizi Gratis, Danseskoa...</td>\n",
       "      <td>tinjau program makan bergizi gratis danseskoal...</td>\n",
       "      <td>[tinjau, program, makan, gizi, gratis, dansesk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>üçΩÔ∏è‚ú® Program Makan Bergizi Gratis dapat meningk...</td>\n",
       "      <td>program makan bergizi gratis dapat meningkatk...</td>\n",
       "      <td>[program, makan, gizi, gratis, tingkat, ekonom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Makin ngga jelas aja ini negara demi makan sia...</td>\n",
       "      <td>makin ngga jelas aja ini negara demi makan sia...</td>\n",
       "      <td>[negara, makan, siang, gratis, lebih baik, per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"pa, siang tadi adek makan gratis.\"\\n\\n\"dek, m...</td>\n",
       "      <td>pa siang tadi adek makan gratis\\n\\ndek malam i...</td>\n",
       "      <td>[pa, siang, adek, makan, gratis, dek, malam, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Makan siang gratis https://t.co/FF7x4tFkyt</td>\n",
       "      <td>makan siang gratis</td>\n",
       "      <td>[makan, siang, gratis]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>Geber Makan Bergizi Gratis, Badan Gizi Butuh R...</td>\n",
       "      <td>geber makan bergizi gratis badan gizi butuh rp...</td>\n",
       "      <td>[geber, makan, gizi, gratis, badan, gizi, butu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>hasil dari School Meal Program, atau \"Pradan M...</td>\n",
       "      <td>hasil dari school meal program atau pradan man...</td>\n",
       "      <td>[hasil, school, meal, program, pradan, mantri,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>Baznas Siap Dukung Program Makan Bergizi Grati...</td>\n",
       "      <td>baznas siap dukung program makan bergizi grati...</td>\n",
       "      <td>[baznas, dukung, program, makan, gizi, gratis,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>Makan bergizi gratis yang sebenernya gak grati...</td>\n",
       "      <td>makan bergizi gratis yang sebenernya gak grati...</td>\n",
       "      <td>[makan, gizi, gratis, sebenernya, gratis, efis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>Program makan bergizi gratis memberikan manfaa...</td>\n",
       "      <td>program makan bergizi gratis memberikan manfaa...</td>\n",
       "      <td>[program, makan, gizi, gratis, manfaat, penuh,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>843 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Tweet  \\\n",
       "0    Tinjau Program Makan Bergizi Gratis, Danseskoa...   \n",
       "1    üçΩÔ∏è‚ú® Program Makan Bergizi Gratis dapat meningk...   \n",
       "2    Makin ngga jelas aja ini negara demi makan sia...   \n",
       "3    \"pa, siang tadi adek makan gratis.\"\\n\\n\"dek, m...   \n",
       "4           Makan siang gratis https://t.co/FF7x4tFkyt   \n",
       "..                                                 ...   \n",
       "838  Geber Makan Bergizi Gratis, Badan Gizi Butuh R...   \n",
       "839  hasil dari School Meal Program, atau \"Pradan M...   \n",
       "840  Baznas Siap Dukung Program Makan Bergizi Grati...   \n",
       "841  Makan bergizi gratis yang sebenernya gak grati...   \n",
       "842  Program makan bergizi gratis memberikan manfaa...   \n",
       "\n",
       "                                            Clean Text  \\\n",
       "0    tinjau program makan bergizi gratis danseskoal...   \n",
       "1     program makan bergizi gratis dapat meningkatk...   \n",
       "2    makin ngga jelas aja ini negara demi makan sia...   \n",
       "3    pa siang tadi adek makan gratis\\n\\ndek malam i...   \n",
       "4                                  makan siang gratis    \n",
       "..                                                 ...   \n",
       "838  geber makan bergizi gratis badan gizi butuh rp...   \n",
       "839  hasil dari school meal program atau pradan man...   \n",
       "840  baznas siap dukung program makan bergizi grati...   \n",
       "841  makan bergizi gratis yang sebenernya gak grati...   \n",
       "842  program makan bergizi gratis memberikan manfaa...   \n",
       "\n",
       "                                            Preprocess  \n",
       "0    [tinjau, program, makan, gizi, gratis, dansesk...  \n",
       "1    [program, makan, gizi, gratis, tingkat, ekonom...  \n",
       "2    [negara, makan, siang, gratis, lebih baik, per...  \n",
       "3    [pa, siang, adek, makan, gratis, dek, malam, m...  \n",
       "4                               [makan, siang, gratis]  \n",
       "..                                                 ...  \n",
       "838  [geber, makan, gizi, gratis, badan, gizi, butu...  \n",
       "839  [hasil, school, meal, program, pradan, mantri,...  \n",
       "840  [baznas, dukung, program, makan, gizi, gratis,...  \n",
       "841  [makan, gizi, gratis, sebenernya, gratis, efis...  \n",
       "842  [program, makan, gizi, gratis, manfaat, penuh,...  \n",
       "\n",
       "[843 rows x 3 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "55852e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = {\"sehat\", \"dukung\", \"presiden\", \"program\", \"bantu\", \"sejahtera\", \"tingkat\", \"prioritas\", \"hidup\", \"generasi\", \"emas\", \"murah\", \"ceria\", \"nikmat\", \"enak\", \"pintar\", \"sukses\"}\n",
    "negative_words = {\"tolak\", \"anggar\", \"tidak\", \"salah\", \"butuh\", \"miskin\", \"kena\", \"demo\", \"bayar\", \"stunting\", \"udah\", \"dapet\", \"sampe\", \"janji\", \"duit\", \"cemas\", \"hutang\", \"potong\", \"pangkas\", \"hilang\", \"tapi\", \"mahal\", \"bodoh\", \"hancur\", \"efisiensi\", \"antem\", \"kritis\", \"anggur\", \"ancam\", \"bohong\", \"takut\", \"bubar\", \"korupsi\", \"omonomon\", \"tai\", \"anjing\", \"babi\", \"bohong\", \"gelap\", \"indonesiagelap\", \"defisit\", \"phk\", \"lupa\", \"paksa\", \"malas\", \"berat\", \"konyol\"}\n",
    "\n",
    "def determine_sentiment(words):\n",
    "\n",
    "    if isinstance(words, list):\n",
    "        positive_count = sum(1 for word in words if word in positive_words)\n",
    "        negative_count = sum(1 for word in words if word in negative_words)\n",
    "        sentiment_score = positive_count - negative_count\n",
    "\n",
    "        if sentiment_score == 0:\n",
    "            sentiment = \"Netral\"\n",
    "        elif sentiment_score > 0:\n",
    "            sentiment = \"Positif\"\n",
    "        else:\n",
    "            sentiment = \"Negatif\"\n",
    "\n",
    "        return sentiment_score, sentiment\n",
    "    return 0, \"Netral\"\n",
    "\n",
    "tweets_df[['Sentiment Score', 'Sentiment']] = tweets_df['Preprocess'].apply(lambda x: pd.Series(determine_sentiment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f120338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Netral     340\n",
      "Positif    292\n",
      "Negatif    211\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiment_count = tweets_df['Sentiment'].value_counts()\n",
    "print(sentiment_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "78b1cea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_text(tokenized_column):\n",
    "    return tokenized_column.apply(lambda tokens: \" \".join(tokens))\n",
    "\n",
    "tweets_df['Rejoin Text'] = rejoin_text(tweets_df['Preprocess'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e62cd502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tweets_df['Rejoin Text']\n",
    "y = tweets_df['Sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6e9ec9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b64b89b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(class_weight=&#x27;balanced&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', random_state=42)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7634e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "096754f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8402366863905325\n",
      "Decision Tree Precision: 0.8388051363865289\n",
      "Decision Tree Recall: 0.8402366863905325\n",
      "Decision Tree F1-score: 0.8390947864320644\n",
      "Decision Tree Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Negatif       0.77      0.71      0.74        34\n",
      "      Netral       0.86      0.87      0.86        82\n",
      "     Positif       0.85      0.89      0.87        53\n",
      "\n",
      "    accuracy                           0.84       169\n",
      "   macro avg       0.83      0.82      0.82       169\n",
      "weighted avg       0.84      0.84      0.84       169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import  accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))\n",
    "print(\"Decision Tree Precision:\", precision_score(y_test, dt_pred, average='weighted'))\n",
    "print(\"Decision Tree Recall:\", recall_score(y_test, dt_pred, average='weighted'))\n",
    "print(\"Decision Tree F1-score:\", f1_score(y_test, dt_pred, average='weighted'))\n",
    "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, dt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "972759ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(vectorizer, vectorizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9df07ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Tweet:  Makan bergizi gratis buat rakyat kesusahan tidak membantu sama sekali\n",
      "Sentiment:  ['Netral']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('DecisionTreeModel.pkl', 'wb') as model_file:\n",
    "    pickle.dump(dt_model, model_file)\n",
    "\n",
    "with open('DecisionTreeModel.pkl', 'rb') as model_file:\n",
    "    loaded_dt_model = pickle.load(model_file)\n",
    "\n",
    "input_text = input(\"Masukkan text anda\")\n",
    "\n",
    "clean_input = text_cleaning(input_text)\n",
    "preprocess_tokens = preprocess(clean_input, slang_dict)\n",
    "preprocess_input = \"\".join(preprocess_tokens)\n",
    "vectorize_input = vectorizer.transform([preprocess_input])\n",
    "pred_input = loaded_dt_model.predict(vectorize_input)\n",
    "print(f'Your Tweet: ', input_text)\n",
    "print(f'Sentiment: ', pred_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
